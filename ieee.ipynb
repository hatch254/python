{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjToBJoGNcgD02Q8PS5w/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatch254/python/blob/main/ieee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ DATASET"
      ],
      "metadata": {
        "id": "IuVQnbuuL57N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dmebJrF2LpPb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as nm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
        "from sklearn.feature_selection import RFECV\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import multiprocessing\n",
        "\n",
        "import re\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.warn(\"this will not show\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.display.max_rows = 1000\n",
        "pd.options.display.max_columns = 1000\n",
        "pd.options.display.max_colwidth = 1000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbzsKVdjL_yo",
        "outputId": "883b403d-2946-49e0-8894-d767767383ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "x6HvllzfMNKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a081d50-30be-4920-b02b-128997b7535d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 4-Modes-Operations.gslides\n",
            "'AUTHENTICATION APPLICATIONS.gdoc'\n",
            "'Authentication Applications.gslides'\n",
            " Bachelors_Cert.pdf\n",
            "'BBIT Year 4 2019-2020.gdoc'\n",
            "'BBIT Year 4 2019-2020.pdf'\n",
            "'BPM Group1'\n",
            " Classroom\n",
            " Cloud_Computing.docx\n",
            " Colab_Notebooks\n",
            "'EVOLUTION OF THE INDUSTRIAL AGE.gslides'\n",
            "'Net Zero Project.gslides'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Colab_Notebooks/IEEE/train_transaction.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G_hcOYdDjjc",
        "outputId": "5e51d9e4-36da-4786-a099-01ecf6867675"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/IEEE/train_transaction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FOR SPEED WE WILL COPY THE FILES TO THE VM OF GOOGLE COLAB IN THE DRIVE/DISK SPACE GIVEN\n",
        "#WE WILL NAME THIS FOLDER AS /CONTENT/INPUT .. THE DESTINATION DIRECTORY\n",
        "#WE ARE USING THE OS LIBRARY TO READ AND WRITE THE GOOGLE COLAB RESOURCES ENVIRONMENT\n",
        "\n",
        "%%time\n",
        "\n",
        "import os, shutil, zipfile\n",
        "\n",
        "#configuring/initializing the source folder in Drive\n",
        "source_dir = \"/content/drive/MyDrive/Colab_Notebooks/IEEE\"\n",
        "\n",
        "#the four expected files in the source folder\n",
        "files = ['test_identity.csv',\n",
        "         'test_transaction.csv',\n",
        "         'train_identity.csv',\n",
        "         'train_transaction.csv']\n",
        "\n",
        "#configuring the vm directory\n",
        "destination_dir = \"/content/input\"\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "#------- verify each file existance in Drive --------------\n",
        "def missing(source_dir, files):\n",
        "  missing =[] #this is a null or empty list\n",
        "  for name in files:\n",
        "    source_path = os.path.join(source_dir, name)\n",
        "\n",
        "    #check existence of file in path if truthy\n",
        "    if not os.path.exists(source_path):\n",
        "      missing.append(source_path)\n",
        "\n",
        "  #checks if missing list is not empty\n",
        "  if missing:\n",
        "    raise FileNotFoundError(\n",
        "        \"The following file is not found int Drive:\\n- \"+\"\\n-\".join(missing)\n",
        "        +\"\\n\\n Check the path in SRC_DIR and the filename in the files.\")\n",
        "\n",
        "  print(\"All source files are found in Drive\")\n",
        "\n",
        "missing(source_dir, files)\n",
        "\n",
        "#------ Copy from Drive to the local vm for faster input/output ---------\n",
        "for name in files:\n",
        "  src = os.path.join(source_dir, name)\n",
        "  dst = os.path.join(destination_dir, name)\n",
        "\n",
        "  if not os.path.exists(dst): #check existence of file in path if truthy\n",
        "    shutil.copy2(src, dst)\n",
        "    print(f\"Copied -> {dst}\")\n",
        "  else:\n",
        "    print(f\"Already exists -> {dst}\")\n",
        "\n",
        "# ----------- Now read fast from the vm local disk ------------\n",
        "def read_csv_auto(path: str) -> pd.DataFrame:\n",
        "  #If path ends with .zip and contains a single csv, read the csv.\n",
        "  #If path ends with .csv, read directly.\n",
        "  if path.lower().endswith(\".zip\"):\n",
        "    with zipfile.ZipFile(path) as zf:\n",
        "      #first find the csv inside the zip\n",
        "      csv_files = [m for m in zf.namelist() if m.lower().endswith(\".csv\")]\n",
        "      if not csv_files:\n",
        "        raise ValueError(f\"No CSV file in ZIP: {path}\")\n",
        "\n",
        "      #Prefer a file that matches the zip basename, fallback to the first csv\n",
        "      base = os.path.splitext(os.path.basename(path))[0].lower()\n",
        "      pick = None\n",
        "      for m in csv_files:\n",
        "        if base in os.path.basename(m).lower():\n",
        "          pick = m\n",
        "          break\n",
        "      if pick is None:\n",
        "        pick = csv_files[0]\n",
        "      with zf.open(pick) as f:\n",
        "        return pd.read_csv(f, low_memory=True)\n",
        "  else:\n",
        "    return pd.read_csv(path, low_memory=True)\n",
        "\n",
        "\n",
        "#--------Load into DataFrames..........\n",
        "local_paths = {name: os.path.join(destination_dir, name) for name in files}\n",
        "\n",
        "print (\"\\n Loading DAtaFrames from /content ... \")\n",
        "test_id = read_csv_auto(local_paths[\"test_identity.csv\"])\n",
        "test_tr = read_csv_auto(local_paths[\"test_transaction.csv\"])\n",
        "train_id = read_csv_auto(local_paths[\"train_identity.csv\"])\n",
        "train_tr = read_csv_auto(local_paths[\"train_transaction.csv\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHLv0RxaOhNh",
        "outputId": "198500d4-a916-4476-ee5b-7e75e6dde49c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All source files are found in Drive\n",
            "Copied -> /content/input/test_identity.csv\n",
            "Copied -> /content/input/test_transaction.csv\n",
            "Copied -> /content/input/train_identity.csv\n",
            "Copied -> /content/input/train_transaction.csv\n",
            "\n",
            " Loading DAtaFrames from /content ... \n",
            "CPU times: user 52.9 s, sys: 20.7 s, total: 1min 13s\n",
            "Wall time: 1min 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The .shape returns a tuple in the format (number of rows, number of columns)\n",
        "# A tuple is a number/dimensions of a dataframe\n",
        "\n",
        "print(\"\\n Shapes \")\n",
        "print(\"test_id\", test_id.shape)\n",
        "print(\"test_tr\", test_tr.shape)\n",
        "print(\"train_id\", train_id.shape)\n",
        "print(\"train_tr\", train_tr.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xlumn6NMgl0",
        "outputId": "7d219274-2b62-4eb6-884a-12698cbc5638"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Shapes \n",
            "test_id (141907, 41)\n",
            "test_tr (506691, 393)\n",
            "train_id (144233, 41)\n",
            "train_tr (590540, 394)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_tr.merge(train_id, how='left', on='TransactionID')\n",
        "test = test_tr.merge(test_id, how='left', on='TransactionID')\n",
        "\n",
        "print(\"train\", train.shape)\n",
        "print(\"test\", test.shape)\n",
        "\n",
        "#Deleting the rows that are not needed\n",
        "del(train_id, train_tr, test_id, test_tr)\n",
        "#gabbage collector. Collects the deleted data so as to free up RAM memory\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "3NWIXVxsXlHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce90ee1-7a87-4153-c9e8-90ae2a76bfad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train (590540, 434)\n",
            "test (506691, 433)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The train dataset has {train.shape[0]} rows and {train.shape[1]} columns\")\n",
        "print(f\"The test dataset has {test.shape[0]} rows and {test.shape[1]} columns\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6NtpulENakL",
        "outputId": "3fa14bb0-3921-408f-b090-532c92acde9a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The train dataset has 590540 rows and 434 columns\n",
            "The test dataset has 506691 rows and 433 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUq6LszdTUzR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}